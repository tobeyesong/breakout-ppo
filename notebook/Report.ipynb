{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toan Lam**  \n",
    "**GitHub Repo:** https://github.com/tobeyesong/ppo-breakout/  \n",
    "\n",
    "# Reinforcement Learning for Atari Breakout using PPO\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project implements and evaluates a Proximal Policy Optimization (PPO) algorithm to play the Atari game Breakout. The goal was to train an agent that could effectively learn the game's mechanics and develop a strategy to maximize score. The implementation uses PyTorch, Stable-Baselines3, and Gymnasium to create a scalable training pipeline that leverages parallel environments for efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Description\n",
    "\n",
    "### Breakout Game Rules\n",
    "\n",
    "Breakout is a classic Atari game where the player controls a paddle at the bottom of the screen to bounce a ball upward. The objective is to break all the bricks arranged in rows at the top of the screen by hitting them with the ball.\n",
    "\n",
    "Key game elements:\n",
    "- **Paddle**: Player-controlled horizontal bar at the bottom that can move left and right\n",
    "- **Ball**: Bounces off walls, bricks, and the paddle\n",
    "- **Bricks**: Arranged in rows at the top; disappear when hit by the ball\n",
    "- **Scoring**: Player earns points for each brick destroyed\n",
    "- **Lives**: Player loses a life when the ball falls below the paddle\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "For this project, we used the `BreakoutNoFrameskip-v4` environment from Gymnasium's Atari suite. This environment:\n",
    "- Provides raw pixel observations (210×160×3 RGB images)\n",
    "- Discrete action space with 4 possible actions: NOOP, FIRE (start game), RIGHT, LEFT\n",
    "- Terminates episodes after losing all lives\n",
    "- Awards varying points based on brick position (higher rows = more points)\n",
    "\n",
    "We applied standard preprocessing techniques including:\n",
    "- Grayscale conversion and downsizing to 84×84 pixels\n",
    "- Frame stacking (4 consecutive frames) to capture motion\n",
    "- Frame skipping to improve training efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Training Approach\n",
    "\n",
    "### Why PPO?\n",
    "\n",
    "I chose Proximal Policy Optimization (PPO) for this task because:\n",
    "\n",
    "1. **Sample Efficiency**: PPO achieves good performance with fewer environment interactions compared to simpler methods like DQN\n",
    "2. **Stability**: The \"proximal\" constraint prevents large policy updates that could destabilize training\n",
    "3. **Continuous Learning**: PPO's actor-critic structure allows for both discrete actions and efficient value estimation\n",
    "4. **Parallelization**: Easy to implement with multiple parallel environments for faster training\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "The PPO implementation uses Stable-Baselines3 with the following key hyperparameters:\n",
    "\n",
    "```python\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",  # CNN-based policy for image processing\n",
    "    train_env,\n",
    "    learning_rate=2.5e-4,  # Lower = stable but slower learning\n",
    "    n_steps=128,  # Steps per env before update\n",
    "    batch_size=256,  # Minibatch size for gradient updates\n",
    "    n_epochs=4,  # Update passes through each batch\n",
    "    gamma=0.99,  # Discount factor for future rewards\n",
    "    gae_lambda=0.95,  # Balances bias/variance in advantage estimation\n",
    "    clip_range=0.1,  # Limits policy update size for stability\n",
    "    ent_coef=0.01,  # Encourages exploration\n",
    "    device=device,\n",
    ")\n",
    "```\n",
    "\n",
    "The CNN policy network processes frame-stacked images through convolutional layers followed by fully connected layers that output both action probabilities (policy) and state value estimates (critic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The agent was trained using 8 parallel environments to accelerate the data collection process. Training ran for a total of 10 million timesteps with the following setup:\n",
    "\n",
    "- **Hardware**: CUDA-enabled GPU for neural network training\n",
    "- **Parallel Environments**: 8 copies of Breakout running simultaneously\n",
    "- **Training Duration**: 10M timesteps (approximately 9 hours of training)\n",
    "- **Checkpointing**: Models saved periodically for evaluation and resumption\n",
    "- **Monitoring**: TensorBoard logs for tracking metrics\n",
    "\n",
    "### Challenges & Solutions\n",
    "\n",
    "#### Challenge 1: Training Instability\n",
    "\n",
    "Initially, the agent showed unstable learning patterns with high variance in performance between evaluation runs. \n",
    "\n",
    "**Solution**: \n",
    "- Reduced the learning rate from 5e-4 to 2.5e-4 to make updates more conservative\n",
    "- Increased batch size from 128 to 256 to improve gradient estimates\n",
    "- Added entropy coefficient (0.01) to encourage exploration\n",
    "\n",
    "#### Challenge 2: Slow Learning Progress\n",
    "\n",
    "The agent struggled to consistently hit bricks in the early stages of training.\n",
    "\n",
    "**Solution**:\n",
    "- Increased parallel environments from 4 to 8 for more diverse experience collection\n",
    "- Adjusted gamma to 0.99 to better account for delayed rewards\n",
    "- Implemented proper frame stacking to help the agent understand ball dynamics\n",
    "\n",
    "#### Challenge 3: Environment Compatibility\n",
    "\n",
    "Faced issues with the latest Gymnasium API changes affecting frame stacking.\n",
    "\n",
    "**Solution**:\n",
    "- Updated from `FrameStack` to `FrameStackObservation` to match the current API\n",
    "- Made evaluation scripts compatible with both training and visualization needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Analysis\n",
    "\n",
    "### Training Progress\n",
    "\n",
    "The charts below show the agent's learning progress over training:\n",
    "\n",
    "![Episode Mean Rewards](rollout-ep-rew-mean-1.png)\n",
    "*Figure 1: Episode mean rewards over training time showing steady improvement*\n",
    "\n",
    "![Entropy Loss](train-entropy-loss.png)\n",
    "*Figure 2: Entropy loss demonstrating exploration-exploitation balance*\n",
    "\n",
    "![Policy Gradient Loss](train-policy-gradient-loss.png)\n",
    "*Figure 3: Policy gradient loss showing training optimization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "Based on the TensorBoard data from your training session, the agent showed impressive learning progress during training:\n",
    "\n",
    "- **Final Training Reward**: 352.28 points\n",
    "- **Maximum Training Reward**: 360.08 points\n",
    "- **Initial Rewards (first few episodes)**: 1.29 to 3.65 points\n",
    "- **Final Rewards (last few episodes)**: 343.45 to 352.28 points\n",
    "- **Evaluation Reward**: 387.60 points at 80,000 steps\n",
    "\n",
    "For a comprehensive evaluation, the agent was tested over 100 episodes using the headless evaluation script, achieving these results:\n",
    "\n",
    "- **Average Score**: 341.99 ± 100.02\n",
    "- **Median Score**: 387.00\n",
    "- **Min/Max Score**: 25.00/437.00\n",
    "- **Average Episode Length**: 2812.71 steps\n",
    "\n",
    "The high average score of 341.99 confirms the agent's strong performance, while the relatively large standard deviation (±100.02) indicates some variability between episodes. The impressive maximum score of 437.00 demonstrates the agent's potential when conditions are favorable. The large gap between minimum (25.00) and maximum scores suggests that occasional poor episodes still occur, which is common in reinforcement learning due to the inherent randomness in game dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Progression\n",
    "\n",
    "The training data shows a clear progression in the agent's abilities:\n",
    "\n",
    "1. **Initial Phase (0-50k steps)**: The agent started with rewards around 1-3 points, mostly hitting random bricks through trial and error\n",
    "2. **Developing Phase (50k-300k steps)**: Steady improvement as the agent learned to track and return the ball consistently\n",
    "3. **Advanced Phase (300k+ steps)**: Rewards stabilizing above 340 points, indicating the agent developed strategic gameplay\n",
    "\n",
    "This progression can be seen in the reward chart, while the policy gradient and entropy losses show how the learning algorithm gradually converged to an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Demonstration\n",
    "\n",
    "A video showing the agent's performance can be viewed here:\n",
    "[PPO Breakout Agent Gameplay](https://youtu.be/v8D32hMqH4U)\n",
    "\n",
    "The visualization shows how the agent learned to position the paddle strategically to keep the ball in play and target remaining bricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Future Work\n",
    "\n",
    "### What Worked Well\n",
    "\n",
    "- The PPO algorithm proved effective at learning Breakout, developing strategies beyond simple ball tracking\n",
    "- Parallelization significantly improved training efficiency\n",
    "- The CNN architecture successfully processed visual inputs to understand game state\n",
    "\n",
    "### Limitations & Challenges\n",
    "\n",
    "- The agent occasionally struggles with edge cases (very fast ball movement)\n",
    "- Performance plateaued after ~7M steps, suggesting possible diminishing returns\n",
    "- The fixed hyperparameter set may not be optimal for all phases of learning\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Curriculum Learning**: Implement progressive difficulty increase during training\n",
    "2. **Hyperparameter Scheduling**: Dynamically adjust learning rate and exploration parameters\n",
    "3. **Alternative Architectures**: Compare performance with Rainbow DQN or A2C implementations\n",
    "4. **Human Demonstrations**: Incorporate imitation learning from expert human demonstrations\n",
    "5. **Transfer Learning**: Test if skills transfer to similar games like Pong or other Atari titles\n",
    "\n",
    "This project demonstrates that PPO can effectively learn complex visual-based control tasks with delayed rewards. The techniques used here could be extended to more challenging environments or real-world robotics applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Project Repository: [github.com/tobeyesong/ppo-breakout](https://github.com/tobeyesong/ppo-breakout)\n",
    "2. Gameplay Demo: [YouTube - PPO Breakout Agent](https://youtu.be/v8D32hMqH4U)\n",
    "3. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)\n",
    "4. Stable-Baselines3 Documentation: [stable-baselines3.readthedocs.io](https://stable-baselines3.readthedocs.io/)\n",
    "5. Gymnasium Atari Environments: [gymnasium.farama.org/environments/atari/](https://gymnasium.farama.org/environments/atari/)\n",
    "6. Visualization Files (PNG):\n",
    "   - `rollout-ep-rew-mean-1.png`\n",
    "   - `train-entropy-loss.png`\n",
    "   - `train-policy-gradient-loss.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
